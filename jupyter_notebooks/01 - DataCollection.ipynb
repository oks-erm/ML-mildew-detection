{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Download data from Kaggle and prepare it for the next steps\n",
        "\n",
        "* Clean and split the data into train, test and validation sets\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Kaggle JSON file - authentication key\n",
        "\n",
        "* Kaggle API - to download the data\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Train, test and validation sets in `inputs/datasets/cherry_leaves_dataset`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up the environment\n",
        "### Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp38-cp38-macosx_10_9_x86_64.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.1.2\n",
            "  Downloading pandas-1.1.2-cp38-cp38-macosx_10_9_x86_64.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.3.1\n",
            "  Downloading matplotlib-3.3.1-cp38-cp38-macosx_10_9_x86_64.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting seaborn==0.11.0\n",
            "  Downloading seaborn-0.11.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.1/283.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plotly==4.12.0\n",
            "  Downloading plotly-4.12.0-py2.py3-none-any.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting streamlit==0.85.0\n",
            "  Downloading streamlit-0.85.0-py2.py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp38-cp38-macosx_10_13_x86_64.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting tensorflow-cpu==2.6.0\n",
            "  Downloading tensorflow_cpu-2.6.0-cp38-cp38-macosx_10_11_x86_64.whl (199.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
            "\u001b[?25hCollecting keras==2.6.0\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from pandas==1.1.2->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 2)) (2.8.2)\n",
            "Collecting pytz>=2017.2\n",
            "  Downloading pytz-2023.2-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.1/502.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.4-cp38-cp38-macosx_10_9_x86_64.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2020.06.20 in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from matplotlib==3.3.1->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 3)) (2022.12.7)\n",
            "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=6.2.0\n",
            "  Downloading Pillow-9.4.0-2-cp38-cp38-macosx_10_10_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting scipy>=1.0\n",
            "  Downloading scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl (35.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from plotly==4.12.0->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 5)) (1.16.0)\n",
            "Collecting altair>=3.2.0\n",
            "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitpython\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.0 in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from streamlit==0.85.0->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 7)) (6.2)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-11.0.0-cp38-cp38-macosx_10_14_x86_64.whl (24.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting cachetools>=4.0\n",
            "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting click<8.0,>=7.0\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astor\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting validators\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "Collecting tzlocal\n",
            "  Downloading tzlocal-4.3-py3-none-any.whl (20 kB)\n",
            "Collecting protobuf!=3.11,>=3.6.0\n",
            "  Downloading protobuf-4.22.1-cp37-abi3-macosx_10_9_universal2.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.2/397.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from streamlit==0.85.0->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 7)) (23.0)\n",
            "Collecting blinker\n",
            "  Using cached blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting attrs\n",
            "  Using cached attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=0.11\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 10)) (0.38.4)\n",
            "Collecting gast==0.4.0\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting opt-einsum~=3.3.0\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting tensorflow-estimator~=2.6\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astunparse~=1.6.3\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting termcolor~=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting h5py~=3.1.0\n",
            "  Downloading h5py-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting google-pasta~=0.2\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
            "  Downloading grpcio-1.53.0-cp38-cp38-macosx_10_10_universal2.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tensorboard~=2.6\n",
            "  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting entrypoints\n",
            "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Collecting jinja2\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting jsonschema>=3.0\n",
            "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting toolz\n",
            "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.0\n",
            "  Downloading scipy-1.10.0-cp38-cp38-macosx_10_15_x86_64.whl (35.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.3-cp38-cp38-macosx_10_9_x86_64.whl (34.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 10)) (65.6.3)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.0-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.16.3-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp38-cp38-macosx_10_14_x86_64.whl (35 kB)\n",
            "Collecting pytz-deprecation-shim\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from validators->streamlit==0.85.0->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 7)) (5.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp38-cp38-macosx_10_9_x86_64.whl (13 kB)\n",
            "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
            "  Downloading pyrsistent-0.19.3-cp38-cp38-macosx_10_9_universal2.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pkgutil-resolve-name>=1.3.10\n",
            "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
            "Collecting importlib-resources>=1.4.0\n",
            "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 10)) (6.1.0)\n",
            "Collecting tzdata\n",
            "  Downloading tzdata-2023.2-py2.py3-none-any.whl (342 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/oksanaerm/opt/anaconda3/envs/myenv/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt (line 10)) (3.15.0)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Building wheels for collected packages: clang, termcolor, wrapt, validators\n",
            "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30681 sha256=1f51ab63a53f359e27b8594338c3f1c75db3f3df72ddf9ffd3efedcd0dad55a7\n",
            "  Stored in directory: /Users/oksanaerm/Library/Caches/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=9297206b4e3c1eb4d0b264ba12a6f3eefbeaaf66484846131679d6a556753a74\n",
            "  Stored in directory: /Users/oksanaerm/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl size=32957 sha256=6ecca3b94a32e8b8765951fba27abdb49a326c302741858d022f2808a1033c3a\n",
            "  Stored in directory: /Users/oksanaerm/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=99a164ce557e23026ab7c693321726cca61f67cf2f32d1d85ecd44d2176b33ae\n",
            "  Stored in directory: /Users/oksanaerm/Library/Caches/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
            "Successfully built clang termcolor wrapt validators\n",
            "Installing collected packages: wrapt, typing-extensions, termcolor, tensorboard-plugin-wit, pytz, pyasn1, keras, flatbuffers, clang, validators, urllib3, tzdata, toolz, toml, threadpoolctl, tensorflow-estimator, tensorboard-data-server, smmap, six, rsa, pyrsistent, pyparsing, pyasn1-modules, protobuf, pkgutil-resolve-name, pillow, oauthlib, numpy, MarkupSafe, kiwisolver, joblib, importlib-resources, idna, grpcio, gast, entrypoints, cycler, click, charset-normalizer, cachetools, blinker, base58, backports.zoneinfo, attrs, astor, werkzeug, scipy, retrying, requests, pytz-deprecation-shim, pyarrow, opt-einsum, markdown, keras-preprocessing, jsonschema, jinja2, h5py, google-pasta, google-auth, gitdb, astunparse, absl-py, tzlocal, scikit-learn, requests-oauthlib, pydeck, plotly, pandas, matplotlib, gitpython, seaborn, google-auth-oauthlib, altair, tensorboard, streamlit, tensorflow-cpu\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "Successfully installed MarkupSafe-2.1.2 absl-py-0.15.0 altair-4.2.2 astor-0.8.1 astunparse-1.6.3 attrs-22.2.0 backports.zoneinfo-0.2.1 base58-2.1.1 blinker-1.5 cachetools-5.3.0 charset-normalizer-3.1.0 clang-5.0 click-7.1.2 cycler-0.11.0 entrypoints-0.4 flatbuffers-1.12 gast-0.4.0 gitdb-4.0.10 gitpython-3.1.31 google-auth-2.16.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.53.0 h5py-3.1.0 idna-3.4 importlib-resources-5.12.0 jinja2-3.1.2 joblib-1.2.0 jsonschema-4.17.3 keras-2.6.0 keras-preprocessing-1.1.2 kiwisolver-1.4.4 markdown-3.4.3 matplotlib-3.3.1 numpy-1.19.2 oauthlib-3.2.2 opt-einsum-3.3.0 pandas-1.1.2 pillow-9.4.0 pkgutil-resolve-name-1.3.10 plotly-4.12.0 protobuf-4.22.1 pyarrow-11.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydeck-0.8.0 pyparsing-3.0.9 pyrsistent-0.19.3 pytz-2023.2 pytz-deprecation-shim-0.1.0.post0 requests-2.28.2 requests-oauthlib-1.3.1 retrying-1.3.4 rsa-4.9 scikit-learn-0.24.2 scipy-1.9.3 seaborn-0.11.0 six-1.15.0 smmap-5.0.0 streamlit-0.85.0 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorflow-cpu-2.6.0 tensorflow-estimator-2.12.0 termcolor-1.1.0 threadpoolctl-3.1.0 toml-0.10.2 toolz-0.12.0 typing-extensions-3.7.4.3 tzdata-2023.2 tzlocal-4.3 urllib3-1.26.15 validators-0.20.0 werkzeug-2.2.3 wrapt-1.12.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "! pip install -r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change working directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/oksanaerm/ML/ML-mildew-detection/jupyter_notebooks'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/oksanaerm/ML/ML-mildew-detection'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Using cached kaggle-1.5.13.tar.gz (63 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (1.26.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from requests->kaggle) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from requests->kaggle) (2.0.4)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.13-py3-none-any.whl size=77733 sha256=91e79661a108a1caba861877a1d69cc846c25fec6b9b57fb5848dbacd14354b6\n",
            "  Stored in directory: /Users/oksanaerm/Library/Caches/pip/wheels/9c/45/15/6d6d116cd2539fb8f450d64b0aee4a480e5366bb11b42ac763\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "Successfully installed kaggle-1.5.13\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Kaggle configuaration directory to current working directory and permission for kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/oksanaerm/ML/ML-mildew-detection\n"
          ]
        }
      ],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "print(os.environ['KAGGLE_CONFIG_DIR'])\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download data from Kaggle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "Set Kaggle Dataset and Download it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading cherry-leaves.zip to inputs/cherry_leaves_dataset\n",
            "100%|█████████████████████████████████████▉| 55.0M/55.0M [00:08<00:00, 7.55MB/s]\n",
            "100%|██████████████████████████████████████| 55.0M/55.0M [00:08<00:00, 7.01MB/s]\n"
          ]
        }
      ],
      "source": [
        "KaggleDatasetPath = \"codeinstitute/cherry-leaves\"\n",
        "DestinationFolder = \"inputs/cherry_leaves_dataset\"\n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the compressed file, and remove the original file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(DestinationFolder + '/cherry-leaves.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(DestinationFolder)\n",
        "\n",
        "os.remove(DestinationFolder + '/cherry-leaves.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning\n",
        "Remove non image files if any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_image_files(my_data_dir):\n",
        "    \"\"\"\n",
        "    Search through dataset to identify and remove non image files\n",
        "    \"\"\"\n",
        "    image_extensions = ('.png', '.jpg', '.jpeg')\n",
        "    for root, dirs, files in os.walk(my_data_dir):\n",
        "        # os.walk to traverse the directory structure and look for image files\n",
        "        i = 0\n",
        "        j = 0\n",
        "        for file in files:\n",
        "            if not file.lower().endswith(image_extensions):\n",
        "                file_location = os.path.join(\n",
        "                    root, file)  # construct the file path\n",
        "                os.remove(file_location)  # remove non image file\n",
        "                i += 1\n",
        "            else:\n",
        "                j += 1\n",
        "        print(f\"Folder: {root} - has image file(s): {j}\")\n",
        "        print(f\"Folder: {root} - has non-image file(s): {i}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The solution using `os.walk` is time/space efficient because it avoids having to load the entire directory tree into memory at once. Instead, it generates the directory tree as needed, processing each directory and file as it goes.\n",
        "\n",
        "Using os.walk also avoids the need to explicitly concatenate directory and file paths using string operations, which can be slow for large directory structures.\n",
        "\n",
        "It also avoids the need to call `os.path.isdir()` to check whether a file is a directory. This is because os.walk generates only files, not directories, for each directory it visits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves - has image file(s): 0\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves - has non-image file(s): 0\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves/powdery_mildew - has image file(s): 2104\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves/powdery_mildew - has non-image file(s): 0\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves/healthy - has image file(s): 2104\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves/healthy - has non-image file(s): 0\n"
          ]
        }
      ],
      "source": [
        "remove_non_image_files(my_data_dir='inputs/cherry_leaves_dataset/cherry-leaves')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split the data into train, validation, and test sets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "70-10-20 ratio is justified by the fact that the dataset is relatively large, and a 70% training set is a good starting point. A 10% validation set can provide enough data to optimize the model's hyperparameters without overfitting to the training set. Lastly, a 20% test set can provide a reasonable estimate of the model's performance on unseen data.\n",
        "\n",
        "Additionally, this ratio can help in achieving a better balance between the size of the validation/test set and the amount of data available for training. Having a larger validation and test set provides more reliable estimates of the model's generalization performance, which is crucial in ensuring that the model is not overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "def split_train_validation_test_images(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):\n",
        "    \"\"\"\n",
        "    split data set into three groups by ratio's .7, .1, .2\n",
        "    \"\"\"\n",
        "    if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
        "        print(\"train_set_ratio + validation_set_ratio + test_set_ratio should sum to 1.0\")\n",
        "        return\n",
        "\n",
        "    # gets classes labels\n",
        "    labels = os.listdir(my_data_dir)  # it should get only the folder name\n",
        "    if 'test' in labels:\n",
        "        pass\n",
        "    else:\n",
        "        # create train, test folders with classes labels sub-folder\n",
        "        for folder in ['train', 'validation', 'test']:\n",
        "            for label in labels:\n",
        "                os.makedirs(name=my_data_dir + '/' + folder + '/' + label)\n",
        "\n",
        "        for label in labels:\n",
        "\n",
        "            files = os.listdir(my_data_dir + '/' + label)\n",
        "            random.shuffle(files)\n",
        "\n",
        "            train_set_files_qty = int(len(files) * train_set_ratio)\n",
        "            validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
        "\n",
        "            # Move files to appropriate set directories\n",
        "            # Use of enumerate leads to improved memory efficiency and faster execution time,\n",
        "            # particularly in cases where the loop is iterating over a large number of items.\n",
        "            for count, file_name in enumerate(files):\n",
        "                if count < train_set_files_qty:\n",
        "                    # move a given file to the train set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/train/' + label + '/' + file_name)\n",
        "\n",
        "                elif count < (train_set_files_qty + validation_set_files_qty):\n",
        "                    # move a given file to the validation set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/validation/' + label + '/' + file_name)\n",
        "\n",
        "                else:\n",
        "                    # move given file to test set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/test/' + label + '/' + file_name)\n",
        "\n",
        "            os.rmdir(my_data_dir + '/' + label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_train_validation_test_images(my_data_dir=f\"inputs/cherry_leaves_dataset/cherry-leaves\",\n",
        "                                   train_set_ratio=0.7,\n",
        "                                   validation_set_ratio=0.1,\n",
        "                                   test_set_ratio=0.2\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo\n",
        "\n",
        "git add .\n",
        "\n",
        "git commit - m \"Add and prepare cherry leaves dataset\"\n",
        "\n",
        "git push"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "____"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Step\n",
        "    \n",
        "    [02 - Data Visualization.ipynb]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "My Python Environment",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
