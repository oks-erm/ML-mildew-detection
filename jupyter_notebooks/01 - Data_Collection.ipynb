{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Download data from Kaggle and prepare it for the next steps\n",
        "\n",
        "* Clean and split the data into train, test and validation sets\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Kaggle JSON file - authentication key\n",
        "\n",
        "* Kaggle API - to download the data\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Train, test and validation sets in `inputs/datasets/cherry_leaves_dataset`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up the environment\n",
        "### Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -r /Users/oksanaerm/ML/ML-mildew-detection/requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___\n",
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___\n",
        "### Change working directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/oksanaerm/ML/ML-mildew-detection/jupyter_notebooks'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/oksanaerm/ML/ML-mildew-detection'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___\n",
        "### Install Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Using cached kaggle-1.5.13.tar.gz (63 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from kaggle) (1.26.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from requests->kaggle) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/oksanaerm/opt/anaconda3/lib/python3.9/site-packages (from requests->kaggle) (2.0.4)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.13-py3-none-any.whl size=77733 sha256=91e79661a108a1caba861877a1d69cc846c25fec6b9b57fb5848dbacd14354b6\n",
            "  Stored in directory: /Users/oksanaerm/Library/Caches/pip/wheels/9c/45/15/6d6d116cd2539fb8f450d64b0aee4a480e5366bb11b42ac763\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "Successfully installed kaggle-1.5.13\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Kaggle configuaration directory to current working directory and permission for kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/oksanaerm/ML/ML-mildew-detection\n"
          ]
        }
      ],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "print(os.environ['KAGGLE_CONFIG_DIR'])\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___\n",
        "# Download data from Kaggle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "Set Kaggle Dataset and Download it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading cherry-leaves.zip to inputs/cherry_leaves_dataset\n",
            "100%|█████████████████████████████████████▉| 55.0M/55.0M [00:08<00:00, 7.55MB/s]\n",
            "100%|██████████████████████████████████████| 55.0M/55.0M [00:08<00:00, 7.01MB/s]\n"
          ]
        }
      ],
      "source": [
        "KaggleDatasetPath = \"codeinstitute/cherry-leaves\"\n",
        "DestinationFolder = \"inputs/cherry_leaves_dataset\"\n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the compressed file, and remove the original file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(DestinationFolder + '/cherry-leaves.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(DestinationFolder)\n",
        "\n",
        "os.remove(DestinationFolder + '/cherry-leaves.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning\n",
        "Remove non image files if any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_image_files(my_data_dir):\n",
        "    \"\"\"\n",
        "    Search through dataset to identify and remove non image files\n",
        "    \"\"\"\n",
        "    image_extensions = ('.png', '.jpg', '.jpeg')\n",
        "    for root, dirs, files in os.walk(my_data_dir):\n",
        "        # os.walk to traverse the directory structure and look for image files\n",
        "        i = 0\n",
        "        j = 0\n",
        "        for file in files:\n",
        "            if not file.lower().endswith(image_extensions):\n",
        "                file_location = os.path.join(\n",
        "                    root, file)  # construct the file path\n",
        "                os.remove(file_location)  # remove non image file\n",
        "                i += 1\n",
        "            else:\n",
        "                j += 1\n",
        "        print(f\"Folder: {root} - has image file(s): {j}\")\n",
        "        print(f\"Folder: {root} - has non-image file(s): {i}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The solution using `os.walk` is time/space efficient because it avoids having to load the entire directory tree into memory at once. Instead, it generates the directory tree as needed, processing each directory and file as it goes.\n",
        "\n",
        "Using os.walk also avoids the need to explicitly concatenate directory and file paths using string operations, which can be slow for large directory structures.\n",
        "\n",
        "It also avoids the need to call `os.path.isdir()` to check whether a file is a directory. This is because os.walk generates only files, not directories, for each directory it visits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves - has image file(s): 0\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves - has non-image file(s): 0\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves/powdery_mildew - has image file(s): 2104\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves/powdery_mildew - has non-image file(s): 0\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves/healthy - has image file(s): 2104\n",
            "Folder: inputs/cherry_leaves_dataset/cherry-leaves/healthy - has non-image file(s): 0\n"
          ]
        }
      ],
      "source": [
        "remove_non_image_files(my_data_dir='inputs/cherry_leaves_dataset/cherry-leaves')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split the data into train, validation, and test sets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "70-10-20 ratio is justified by the fact that the dataset is relatively large, and a 70% training set is a good starting point. A 10% validation set can provide enough data to optimize the model's hyperparameters without overfitting to the training set. Lastly, a 20% test set can provide a reasonable estimate of the model's performance on unseen data.\n",
        "\n",
        "Additionally, this ratio can help in achieving a better balance between the size of the validation/test set and the amount of data available for training. Having a larger validation and test set provides more reliable estimates of the model's generalization performance, which is crucial in ensuring that the model is not overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "def split_train_validation_test_images(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):\n",
        "    \"\"\"\n",
        "    split data set into three groups by ratio's .7, .1, .2\n",
        "    \"\"\"\n",
        "    if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
        "        print(\"train_set_ratio + validation_set_ratio + test_set_ratio should sum to 1.0\")\n",
        "        return\n",
        "\n",
        "    # gets classes labels\n",
        "    labels = os.listdir(my_data_dir)  # it should get only the folder name\n",
        "    if 'test' in labels:\n",
        "        pass\n",
        "    else:\n",
        "        # create train, test folders with classes labels sub-folder\n",
        "        for folder in ['train', 'validation', 'test']:\n",
        "            for label in labels:\n",
        "                os.makedirs(name=my_data_dir + '/' + folder + '/' + label)\n",
        "\n",
        "        for label in labels:\n",
        "\n",
        "            files = os.listdir(my_data_dir + '/' + label)\n",
        "            random.shuffle(files)\n",
        "\n",
        "            train_set_files_qty = int(len(files) * train_set_ratio)\n",
        "            validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
        "\n",
        "            # Move files to appropriate set directories\n",
        "            # Use of enumerate leads to improved memory efficiency and faster execution time,\n",
        "            # particularly in cases where the loop is iterating over a large number of items.\n",
        "            for count, file_name in enumerate(files):\n",
        "                if count < train_set_files_qty:\n",
        "                    # move a given file to the train set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/train/' + label + '/' + file_name)\n",
        "\n",
        "                elif count < (train_set_files_qty + validation_set_files_qty):\n",
        "                    # move a given file to the validation set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/validation/' + label + '/' + file_name)\n",
        "\n",
        "                else:\n",
        "                    # move given file to test set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/test/' + label + '/' + file_name)\n",
        "\n",
        "            os.rmdir(my_data_dir + '/' + label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_train_validation_test_images(my_data_dir=f\"inputs/cherry_leaves_dataset/cherry-leaves\",\n",
        "                                   train_set_ratio=0.7,\n",
        "                                   validation_set_ratio=0.1,\n",
        "                                   test_set_ratio=0.2\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to GitHub\n",
        "\n",
        "git add .\n",
        "\n",
        "git commit -m \"Add and prepare cherry leaves dataset\"\n",
        "\n",
        "git push"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "____"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Step\n",
        "    \n",
        "* [02 - Data Visualization.ipynb](02%20-%20Data_Visualization.ipynb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5 (v3.10.5:f377153967, Jun  6 2022, 12:36:10) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
